<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Jian Xu">
  <meta name="description" content="Jian Xu's Homepage">
  <meta name="keywords" content="Jian Xu,徐健,homepage,主页,PhD,Large Multimodal Models, AI4Science, Pose Estimation, Image Retrieval">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jian Xu (徐健)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jian Xu (徐健)</name>
              </p>
              <p style="text-align:center">
                <span>&#9993;</span> chenxy.sean[at]gmail.com &nbsp;
              </p>
              <p style="text-align:left">
                <a href="https://scholar.google.com.hk/citations?user=hqyYoO0AAAAJ&hl=zh-CN"><img src="asserts/google.png" height="40px"></a> &nbsp;&nbsp;
                <a href="https://github.com/XJhaoren"><img src="asserts/github.png" height="40px"></a> &nbsp;&nbsp;
                <!-- <a href="asserts/CV.pdf"><img src="asserts/resume.png" height="40px"></a> &nbsp; -->
              </p>
              <p>I am currently a Postdoctoral Research Fellow at Peking University.
              </p>
              <p>Before joining PKU, I have 3.5 years of experience in AI corporations, Kuaishou and Xiaobing, working closely with <a href="https://sites.google.com/site/zjuwby/">Baoyaun Wang</a>.
              </p>
              <p>
                I obtained my Ph.D. in Control Science and Engineering from Institute of Automation, Chinese Academy of Science in 2020, under the supervision of 
                <a href="https://www.coe.pku.edu.cn/teaching/all_time/7292.html">Prof. Junzhi Yu</a>. 
                Previously I received my B.S. in Electrical Engineering and Automation from
                Chengdu University of Technology in 2015.
              </p>
              <p>
                <!-- I have been focusing on object detection/tracking since I started my PhD in 2015.  -->
                My research interests lie in human-centric modeling, understanding, interaction, and intelligence.
                <!-- joint field of robotics, graphics, and computer vision, 
                including but not limited to human digitalization, 3D geometry understanding, human-machine interaction, and robot perception. -->
                </p>
              <p>
                <!-- <strong>
                I'm seeking research interns on 3D human representation. Feel free to send me an email if you are interested.
                </strong> -->
              </p>
		    
            </td>
            <td style="padding:5% 0% 0% 5%;width:40%;max-width:40%">
              <a href="asserts/protrait.png"><img style="width:100%;max-width:100%" alt="profile photo" src="asserts/protrait.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0% 0% 0% 0%;width:40%;max-width:40%">
            <a href="asserts/logo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="asserts/logo.png" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>2023/08/25</strong> &nbsp Had one papers accepted by IEEE Transactions on Image Processing about visual restoration.
              </p><p>
              <p>
                <strong>2023/07/14</strong> &nbsp Had two papers accepted by ICCV 2023 about 3D-aware GAN and face swap.
              </p><p>
              <p>
                <strong>2023/02/28</strong> &nbsp Had one paper accepted by CVPR 2023 about neural hand rendering.
              </p><p>
              <p>
                <strong>2022/07/09</strong> &nbsp Had one paper accepted by ECCV 2022 about open-world object detection.
              </p><p>
              <p>
                <strong>2022/06/30</strong> &nbsp Had one paper accepted by ACM MM 2022 about hand mesh reconstruction.
              </p><p>
                <strong>2022/04/02</strong> &nbsp Our <a href="https://www.amazon.com/Visual-Perception-Control-Underwater-Robots-dp-0367695782/dp/0367695782/ref=mt_other?_encoding=UTF8&me=&qid=">book</a> was <strong>Highly Recommended</strong> by <a href="https://www.choice360.org/products/choice-reviews/">CHOICE Reviews</a>.              
              </p><p>
                <strong>2022/03/02</strong> &nbsp Had one paper accepted by CVPR 2022 about monocular hand reconstruction.              
              </p><p>
                <strong>2022/02/25</strong> &nbsp Had one paper accepted by IEEE Transactions on Cybernetics about few-shot object detection.              
              </p><p>
                <strong>2021/09/01</strong> &nbsp  <a href="https://github.com/SeanChenxy/Hand3DResearch">Hand3DResearch</a> was released to track recent works in 3D hand tasks.              
              </p><p>
                <strong>2021/03/01</strong> &nbsp Had one paper accepted by CVPR 2021 about monocular hand reconstruction.              
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent Publications</heading>
            <p>
              (<a href="publication.html">here</a> for full publications)
            </p>
          </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                          <source src='asserts/havefun.mp4'>
              </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Xihe Yang*, <strong>Jian Xu</strong> <span>&#9993;</span>*, Shaohui Wang, Daiheng Gao, Xiaoguang Han, Baoyuan Wang <span>&#9993;</span>  (* for equal contribution, <span>&#9993;</span> for corresponding author)
              <br>
              <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.15672">[PDF]</a>
              <a href="https://seanchenxy.github.io/HaveFunWeb/">[Project]</a>
				      <a href="bibtext/havefun.txt">[BibTeX]</a>
              <br>
              <p>Our HaveFun framework can create human avatars from few-shot unconstrained images.</p>
          </td>
        </tr>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                          <source src='asserts/mimic3d.mp4'>
              </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation</papertitle>
              <br>
              <strong>Jian Xu</strong>*, Yu Deng*, Baoyuan Wang  (* for equal contribution)
              <br>
              <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023.
              <br>
              <a href="https://arxiv.org/pdf/2303.09036.pdf">[PDF]</a>
              <a href="https://seanchenxy.github.io/Mimic3DWeb/">[Project]</a>
				      <a href="bibtext/mimic3d.txt">[BibTeX]</a>
              <br>
              <p>We propose a novel 3D-aware GAN that can directly render high-resolution images.</p>
          </td>
        </tr>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                          <source src='asserts/handavatar.mp4'>
              </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video</papertitle>
              <br>
              <strong>Jian Xu</strong>, Baoyuan Wang, Heung-Yeung Shum
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023.
              <br>
              <a href="https://arxiv.org/pdf/2211.12782.pdf">[PDF]</a>
              <a href="https://seanchenxy.github.io/HandAvatarWeb/">[Project]</a>
              <a href="bibtext/handavatar.txt">[BibTeX]</a>
              <br>
              <p>We propose a HandAvatar framework, the first method for neural hand rendering with self-occluded illumination.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
              <img src='asserts/mobrecon.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image</papertitle>
              <br>
              <strong>Jian Xu</strong>, Yufeng Liu, Yajiao Dong, Xiong Zhang, Chongyang Ma, Yanmin Xiong, Yuan Zhang, Xiaoyan Guo
              <br>
              <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022.
              <br>
              <a href="https://arxiv.org/pdf/2112.02753.pdf">[PDF]</a>
              <a href="https://github.com/SeanChenxy/HandMesh">[Code]</a>
              <a href="bibtext/mobrecon.txt">[BibTeX]</a>
              <br>
              <p>We propose a framework for single-view hand mesh reconstruction, which can simultaneously achieve high reconstruction accuracy, fast inference speed, and temporal coherence.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='asserts/cmr.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration</papertitle>
              <br>
              <strong>Jian Xu</strong>, Yufeng Liu, Chongyang Ma, Jianlong Chang, Huayan Wang, Tian Chen, Xiaoyan Guo, Pengfei Wan, Wen Zheng
              <br>
              <em>2021 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2021.
              <br>
              <a href="https://arxiv.org/pdf/2103.02845.pdf">[PDF]</a>
              <a href="https://github.com/SeanChenxy/HandMesh">[Code]</a>
              <a href="bibtext/cmr.txt">[BibTeX]</a>
              <br>
              <p>We reconstruct 3D hand by dividing camera-space mesh recovery into two sub-tasks, i.e., root-relative mesh recovery and root recovery.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='asserts/tdrn.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Joint Anchor-Feature Refinement for Real-Time Accurate Object Detection in Images and Videos</papertitle>
              <br>
              <strong>Jian Xu</strong>, Junzhi Yu, Shihan Kong, Zhengxing Wu, and Li Wen
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2021.
              <br>
              <a href="https://arxiv.org/pdf/1807.08638.pdf">[PDF]</a>
              <a href="bibtext/tdrn.txt">[BibTeX]</a>
              <br>
              <p>We propose a temporal detection method based on anchor and feature offset refinements.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='asserts/ganrs.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards real-time advancement of underwater visual quality with GAN</papertitle>
              <br>
              <strong>Jian Xu</strong>, Junzhi Yu, Shihan Kong, Zhengxing Wu, Xi Fang, and Li Wen
              <br>
              <em>IEEE Transactions on Industrial Electronics (TIE)</em>, 2019.
              <br>
              <a href="https://arxiv.org/pdf/1712.00736.pdf">[PDF]</a>
              <a href="bibtext/ganrs.txt">[BibTeX]</a>
              <br>
              <p>We propose a GAN-RS to elevate underwater visual quality.</p>
          </td>
      </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://yudeng.github.io/">Yu Deng</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
